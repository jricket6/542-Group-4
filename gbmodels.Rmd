---
title: "Gradient Boosted Model(s)"
output: html_document
---

## markdown for gradient boosted models

```{r}
library(tidyverse)
library(caret)
library(xgboost)

salaries <- read.csv("salaries.csv")
```

cleaning to focus data and reduce categorical feature cardinality
only 2023, can account for inflation later
only US for residence, more relevant to class
removing features we do not need and/or have filtered for
```{r}
# salaries <- salaries %>%
#   filter(work_year == 2023, employee_residence == "US")

salaries <- salaries %>%
  filter(work_year == 2023)
  
salaries <- salaries %>%
  select(!(c(work_year, salary, salary_currency, employee_residence)))
```

removing rare job titles (under 1%)
```{r}
job_counts <- salaries %>%
  count(job_title)

job_titles <- as.vector(job_counts[which(job_counts$n > 0.01*nrow(salaries)),1])

salaries <- salaries %>%
  filter(job_title %in% job_titles)

rm(job_counts, job_titles)
```

categorical features as factors
ratio as a factor as well since only (0, 50, 100) represented
```{r}
salaries <- salaries %>%
  mutate(experience_level = as.factor(experience_level),
         employment_type = as.factor(employment_type),
         job_title = as.factor(job_title),
         company_location = as.factor(company_location),
         company_size = as.factor(company_size),
         remote_ratio = as.factor(remote_ratio))
```

remove outliers before making salary categories
```{r}
Q1 <- quantile(salaries$salary_in_usd, 0.25)
Q3 <- quantile(salaries$salary_in_usd, 0.75)
IQR <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

salaries <- salaries[salaries$salary_in_usd >= lower_bound & salaries$salary_in_usd <= upper_bound, ]

rm(IQR, lower_bound, Q1, Q3, upper_bound)
```

one-hot encoding to handle categorical features
```{r}
categorical_data <- salaries[, c("experience_level", "employment_type", "job_title", "remote_ratio","company_location", "company_size")]

dummy_cols <- dummyVars(" ~ .", data = categorical_data)

encoded_data <- predict(dummy_cols, newdata = categorical_data)

clean_salaries <- as.data.frame(cbind(salaries[, c("salary_in_usd")], encoded_data))

colnames(clean_salaries)[1] <- "salary"

rm(categorical_data, dummy_cols, encoded_data)
```

categorizing salaries to a range for classification
```{r}
clean_salaries$salary_category <- cut(clean_salaries$salary, 
                                      breaks = 5,
                                      include.lowest = TRUE,
                                      dig.lab = 10)

clean_salaries$salary <- NULL

table(clean_salaries$salary_category)
levels(clean_salaries$salary_category)

clean_salaries$salary_category <- as.integer(clean_salaries$salary_category) - 1
```

split the data into train and test, remove salary
```{r}
set.seed(11)

split <- createDataPartition(clean_salaries$salary_category, p = 0.7, list = FALSE)

train_data <- clean_salaries[split, ]
test_data <- clean_salaries[-split, ]

train_labels <- train_data$salary_category
test_labels <- test_data$salary_category

train_data <- train_data[, !(colnames(train_data) %in% c("salary_category"))]
test_data <- test_data[, !(colnames(test_data) %in% c("salary_category"))]
```

```{r}
train_matrix <- xgb.DMatrix(data = as.matrix(train_data), label = train_labels)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data), label = test_labels)
```

first model with default parameters cv to find best number of rounds
```{r}
#default parameters
params <- list(booster = "gbtree",
               objective = "multi:softmax",
               num_class = length(unique(train_labels)),
               eta=0.3,
               gamma=0,
               max_depth=6,
               min_child_weight=1,
               subsample=1,
               colsample_bytree=1)

xgbcv <- xgb.cv(params = params,
                data = train_matrix,
                nrounds = 100,
                nfold = 5,
                showsd = T,
                stratified = T,
                print.every.n = 10,
                early.stop.round = 20,
                maximize = F)
```

best iteration checked with cv
```{r}
best_iteration <- 28

final_model <- xgboost(
  data = train_matrix,
  booster = "gbtree",
  objective = "multi:softmax",
  num_class = length(unique(train_labels)),
  eta = 0.3,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1,
  nrounds = best_iteration
)

test_predictions <- predict(final_model, newdata = test_matrix)

mean(test_predictions == test_labels)
```

highly involved grid search for best parameters DONT RUN
```{r}
# Define a grid of parameters to search over
grid_params <- list(
  eta = c(0.1, 0.01, 0.001),
  max_depth = c(4, 6, 8),
  min_child_weight = c(1, 5, 10),
  subsample = c(0.8, 1.0),
  colsample_bytree = c(0.8, 1.0),
  lambda = c(0, 0.1, 1),
  alpha = c(0, 0.1, 1)
)

# Initialize an empty list to store results
results <- list()

# Perform grid search
for (i in seq_len(nrow(expand.grid(grid_params)))) {
  # Combine the default parameters with the current parameter set
  current_params <- c(params, as.list(expand.grid(grid_params)[i, ]))
  
  # Run cross-validation with the current parameter set
  xgbcv <- xgb.cv(
    params = current_params,
    data = train_matrix,
    nrounds = 100,
    nfold = 5,
    showsd = TRUE,
    stratified = TRUE,
    print_every_n = 10,
    early_stopping_rounds = 20,
    maximize = FALSE
  )
  
  # Store the results for later analysis
  results[[i]] <- xgbcv$evaluation_log
}

# Identify the best parameter set
best_params <- names(results)[which.min(sapply(results, function(x) min(x$test_mlogloss.mean)))]

# Retrieve the best parameter set and results
best_params_values <- strsplit(best_params, "_")[[1]]
best_results <- results[[best_params]]

# Print the best parameter set and results
cat("Best Parameters:", best_params, "\n")
print(best_params_values)
print(best_results)


```
