---
title: "Gradient Boosted Model(s)"
output: html_document
---

## markdown for gradient boosted models

```{r}
library(tidyverse)
library(caret)
library(xgboost)
library(ggplot2)
salary <- read.csv("salaries.csv")
```

summary statistics on raw data
```{r}
summary(salary$salary_in_usd)

median_salary <- median(salary$salary_in_usd)

ggplot(salary, aes(x = salary_in_usd)) +
  geom_histogram(binwidth = 10000, fill = "#69b3a2", color = "#e9ecef", alpha = 0.8) +
  geom_vline(xintercept = median_salary, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Salary Distribution",
       x = "Salary in USD",
       y = "Frequency") +
  scale_x_continuous(labels = scales::number_format(scale = 1e-3, suffix = "k")) +
  theme_minimal()

level_mapping <- c("EN" = "Entry-Level", "MI" = "Mid-Level", "SE" = "Senior-Level", "EX" = "Executive-Level")

ggplot(salary, aes(x = factor(experience_level, levels = names(level_mapping)))) +
  geom_bar(fill = "#69b3a2", color = "#e9ecef", alpha = 0.8) +
  labs(title = "Distribution of Experience Level", x = "Experience Level", y = "Frequency") +
  scale_x_discrete(labels = level_mapping) +
  theme_minimal()

ggplot(salary, aes(x = company_size)) +
  geom_bar(fill = "#69b3a2", color = "#e9ecef", alpha = 0.8) +
  labs(title = "Distribution of Company Size", x = "Company Size", y = "Frequency") +
  theme_minimal()

top_job_titles <- salary %>%
  group_by(job_title) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  slice_head(n = 5)

ggplot(top_job_titles, aes(x = fct_reorder(job_title, count), y = count)) +
  geom_bar(stat = "identity", fill = "#69b3a2", color = "#e9ecef", alpha = 0.8) +
  labs(title = "Top 5 Occurring Job Titles", x = "Job Title", y = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
salary <- salary %>%
  filter(work_year %in% c(2022, 2023), company_location == "US")

for (i in 1:nrow(salary)) {
  if (salary$work_year[i] == 2022) {
    salary$salary_in_usd[i] <- salary$salary_in_usd[i] * 1.08
    }
}

rm(i)
```

```{r}
Q1 <- quantile(salary$salary_in_usd, 0.25)

Q3 <- quantile(salary$salary_in_usd, 0.75)

IQR <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR

upper_bound <- Q3 + 1.5 * IQR

salary <- salary[salary$salary_in_usd >= lower_bound & salary$salary_in_usd <= upper_bound, ]

rm(IQR, lower_bound, Q1, Q3, upper_bound)
```

```{r}
salary <- salary %>%
  select(!(c(salary, salary_currency, company_location)))

salary <- salary %>%
  mutate(experience_level = as.factor(experience_level),
         employment_type = as.factor(employment_type),
         job_title = as.factor(job_title),
         company_size = as.factor(company_size),
         employee_residence = as.factor(employee_residence),
         salary_in_usd = as.integer(salary_in_usd))
```

```{r}
categorical_data <- salary[, c("experience_level", "employment_type", "job_title", "employee_residence", "company_size")]

dummy_cols <- dummyVars(" ~ .", data = categorical_data)

encoded_data <- predict(dummy_cols, newdata = categorical_data)

clean_salaries <- as.data.frame(cbind(salary[, c("salary_in_usd")], encoded_data))

colnames(clean_salaries)[1] <- "salary"

rm(categorical_data, dummy_cols, encoded_data)
```

```{r}
set.seed(11)

split <- createDataPartition(clean_salaries$salary, p = 0.7, list = FALSE)

train_data <- clean_salaries[split, ]
test_data <- clean_salaries[-split, ]

train_labels <- train_data$salary
test_labels <- test_data$salary

train_data <- train_data[, !(colnames(train_data) %in% c("salary"))]
test_data <- test_data[, !(colnames(test_data) %in% c("salary"))]
```

```{r}
train_matrix <- xgb.DMatrix(data = as.matrix(train_data), label = train_labels)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data), label = test_labels)
```

```{r}
train_dmatrix <- xgboost::xgb.DMatrix(data = as.matrix(train_data), label = train_labels)
test_dmatrix <- xgboost::xgb.DMatrix(data = as.matrix(test_data), label = test_labels)
```

```{r}
watchlist <- list(train = train_dmatrix, test = test_dmatrix)
ctrl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

model <- train(x = as.matrix(train_data),
               y = train_labels,
               method = "xgbTree",
               trControl = ctrl,
               tuneGrid = expand.grid(nrounds = c(50, 100, 150),
                                      max_depth = c(3, 6, 9),
                                      eta = c(0.1, 0.3, 0.5),
                                      gamma = c(0, 1, 2),
                                      colsample_bytree = c(0.6, 0.8, 1),
                                      min_child_weight = c(1, 3, 5),
                                      subsample = c(0.8, 1)),
               metric = "RMSE",  # Evaluate based on RMSE
               tuneLength = 3,  # Number of combinations to search
               early_stopping_rounds = 10,
               verbose = 2,
               watchlist = watchlist)  # Enable early stopping

model$bestTune

predictions <- predict(model, newdata = as.matrix(test_data))
```

```{r}
rmse <- sqrt(mean((test_labels - predictions)^2))
cat("RMSE:", rmse, "\n")
```

updated grid search parameters to squeeze out more accuracy
```{r}

```
